{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[2주차] XGBoost.ipynb",
      "provenance": [],
      "mount_file_id": "1NA_mV_y4jNFFX7mgPxcHf3fOligNAfge",
      "authorship_tag": "ABX9TyOuiYUZnahgc0Qd3NHeqeEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mori8/2021-winter-kaggle-study/blob/main/week2/%EB%82%A8%EC%88%98%EC%97%B0/%5B2%EC%A3%BC%EC%B0%A8%5D_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jLExYK_Ye5G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwDEadlPupye"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_data(df):\n",
        "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
        "\n",
        "# DecisionTree의 변수중요도\n",
        "def feature_importance(forest, X_train, display_results=True):\n",
        "    ranked_list = []\n",
        "    zero_features = []\n",
        "    \n",
        "    importances = forest.feature_importances_\n",
        "\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    \n",
        "    if display_results:\n",
        "        print(\"Feature ranking:\")\n",
        "\n",
        "    for f in range(X_train.shape[1]):\n",
        "        if display_results:\n",
        "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
        "        \n",
        "        ranked_list.append(X_train.columns[indices[f]])\n",
        "        \n",
        "        if importances[indices[f]] == 0.0:\n",
        "            zero_features.append(X_train.columns[indices[f]])\n",
        "            \n",
        "    return ranked_list, zero_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neh2_F6I7Vzn"
      },
      "source": [
        "# 기존 특성을 바탕으로 새로운 집계 특성 생성\n",
        "def do_features(df):\n",
        "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
        "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
        "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
        "                 ('human_density', 'tamviv', 'rooms'),\n",
        "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
        "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
        "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
        "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
        "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
        "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
        "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
        "                ]\n",
        "    \n",
        "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
        "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
        "\n",
        "    for f_new, f1, f2 in feats_div:\n",
        "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
        "    for f_new, f1, f2 in feats_sub:\n",
        "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
        "    \n",
        "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
        "                'escolari': ['min', 'max', 'mean']\n",
        "               }\n",
        "    \n",
        "    aggs_cat = {'dis': ['mean']}\n",
        "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
        "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
        "            aggs_cat[f_] = ['mean', 'count']\n",
        "\n",
        "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
        "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
        "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
        "        df = df.join(df_agg, how='left', on='idhogar')\n",
        "        del df_agg\n",
        "\n",
        "    df.drop(['Id'], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2AOpiOV7wlN"
      },
      "source": [
        "# one-hot encoding된 필드를 label encoding으로 변환 \n",
        "def convert_OHE2LE(df):\n",
        "    tmp_df = df.copy(deep=True)\n",
        "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
        "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
        "               'instlevel', 'lugar', 'tipovivi',\n",
        "               'manual_elec']:\n",
        "        if 'manual_' not in s_:\n",
        "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
        "        elif 'elec' in s_:\n",
        "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
        "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
        "        # sum이 0인 경우\n",
        "        if 0 in sum_ohe:\n",
        "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
        "                  .format(s_))\n",
        "            col_dummy = s_ + '_dummy'\n",
        "            # 데이터프레임에 dummy column 추가\n",
        "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
        "            # Label Encoding을 위해 column list에 추가 \n",
        "            cols_s_.append(col_dummy)\n",
        "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
        "            if 0 in sum_ohe:\n",
        "                 print(\"The category completion did not work\")\n",
        "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
        "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
        "        if 'parentesco1' in cols_s_:\n",
        "            cols_s_.remove('parentesco1')\n",
        "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
        "    return tmp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-AtNIQC78y9"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/GDSC Sookmyung/캐글 스터디/data/week2/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/GDSC Sookmyung/캐글 스터디/data/week2/test.csv')\n",
        "\n",
        "test_ids = test.Id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDwH8Exx9nK6"
      },
      "source": [
        "def process_df(df_):\n",
        "  encode_data(df_)\n",
        "  # 집계 특성 생성\n",
        "  return do_features(df_)\n",
        "\n",
        "train = process_df(train)\n",
        "test = process_df(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmWxSGap-EAy"
      },
      "source": [
        "# SQBdependency의 제곱근 값으로 dependency Na 값 채우기\n",
        "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
        "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
        "\n",
        "# edjefa = years of education of female head of household\n",
        "# edjefe = years of education of male head of household\n",
        "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "\n",
        "# parentesco1 = 1 if household head\n",
        "# escolari = years of schooling\n",
        "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "# 그래도 남은 yes는 일단 4로..\n",
        "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "# edjefe와 edjefa의 \"no\", \"yes\"를 모두 처리했으니 데이터 타입을 int로 변경\n",
        "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
        "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
        "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
        "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
        "\n",
        "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
        "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
        "\n",
        "# fill some nas\n",
        "train['v2a1']=train['v2a1'].fillna(0)\n",
        "test['v2a1']=test['v2a1'].fillna(0)\n",
        "\n",
        "test['v18q1']=test['v18q1'].fillna(0)\n",
        "train['v18q1']=train['v18q1'].fillna(0)\n",
        "\n",
        "train['rez_esc']=train['rez_esc'].fillna(0)\n",
        "test['rez_esc']=test['rez_esc'].fillna(0)\n",
        "\n",
        "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
        "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
        "\n",
        "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
        "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
        "\n",
        "# v14a = 1 has toilet in the household\n",
        "# sanitario1 = 1 no toilet in the dwelling\n",
        "# abastaguano = 1 if no water provision\n",
        "# 데이터 불일치 수정: 물 공급이 없으면 화장실은 없는 걸로 수정\n",
        "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
        "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
        "\n",
        "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
        "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnUwUk7eQq0j"
      },
      "source": [
        "def train_test_apply_func(train_, test_, func_):\n",
        "    test_['Target'] = 0\n",
        "    # 합친 다음 한 번에 적용\n",
        "    xx = pd.concat([train_, test_])\n",
        "\n",
        "    xx_func = func_(xx)\n",
        "    # 합쳐놓은 데이터프레임 분리\n",
        "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
        "    test_ = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
        "    \n",
        "    del xx, xx_func\n",
        "    return train_, test_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxqNkMqURd0L",
        "outputId": "36de81d2-f0f5-4b28-ec94-a12f722bf64b"
      },
      "source": [
        "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The OHE in techo is incomplete. A new column will be added before label encoding\n",
            "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
            "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhi-LufoNVZ_"
      },
      "source": [
        "## Geo aggregates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2flerUFMRo-7"
      },
      "source": [
        "# convert_OHE2LE()로 생성된 one-hot encoded columns\n",
        "ols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
        "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
        "              'pared_LE']\n",
        "cols_nums = ['age', 'meaneduc', 'dependency', \n",
        "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
        "             'bedrooms', 'overcrowding']\n",
        "\n",
        "def convert_geo2aggs(df_):\n",
        "    # lugar -> 지역 정보\n",
        "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar'] + cols_nums)],\n",
        "                        pd.get_dummies(df_[cols_2_ohe], \n",
        "                                       columns=cols_2_ohe)],axis=1)\n",
        "\n",
        "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
        "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
        "    \n",
        "    del tmp_df\n",
        "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
        "\n",
        "# 지형별 집계 추가\n",
        "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvzQz7KjHUFi"
      },
      "source": [
        "# 각 가정에서 18세 이상인 사람의 수\n",
        "train['num_over_18'] = 0\n",
        "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
        "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
        "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
        "\n",
        "test['num_over_18'] = 0\n",
        "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
        "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
        "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
        "\n",
        "# 다른 커널에서 가져온 features 추가\n",
        "def extract_features(df):\n",
        "    df['bedrooms_to_rooms'] = df['bedrooms'] / df['rooms']\n",
        "    df['rent_to_rooms'] = df['v2a1'] / df['rooms']\n",
        "    df['tamhog_to_rooms'] = df['tamhog'] / df['rooms'] # tamhog - size of the household\n",
        "    df['r4t3_to_tamhog'] = df['r4t3'] / df['tamhog'] # r4t3 - Total persons in the household\n",
        "    df['r4t3_to_rooms'] = df['r4t3'] / df['rooms'] # r4t3 - Total persons in the household\n",
        "    df['v2a1_to_r4t3'] = df['v2a1'] / df['r4t3'] # rent to people in household\n",
        "    df['v2a1_to_r4t3'] = df['v2a1'] / (df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
        "    df['hhsize_to_rooms'] = df['hhsize'] / df['rooms'] # rooms per person\n",
        "    df['rent_to_hhsize'] = df['v2a1'] / df['hhsize'] # rent to household size\n",
        "    df['rent_to_over_18'] = df['v2a1'] / df['num_over_18']\n",
        "    # 18세 이하가 없는 가정의 월세 총합\n",
        "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
        "    \n",
        "extract_features(train)    \n",
        "extract_features(test)   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8JELFlzHWYe"
      },
      "source": [
        "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
        "                 'mobilephone', 'female', ]\n",
        "\n",
        "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
        "\n",
        "needless_cols.extend(instlevel_cols)\n",
        "\n",
        "# 중복된 열 제거\n",
        "train = train.drop(needless_cols, axis=1)\n",
        "test = test.drop(needless_cols, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl9GqGV4HYA7"
      },
      "source": [
        "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
        "    #np.random.seed(seed=seed)\n",
        "    \n",
        "    train2 = train.copy()\n",
        "    \n",
        "    # 무작위로 테스트에 사용할 가구 추출\n",
        "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
        "    \n",
        "    # 무작위로 선택된 가구를 test 데이터로 사용\n",
        "    cv_idx = np.isin(households, cv_hhs)\n",
        "    X_test = train2[cv_idx]\n",
        "    y_test = y[cv_idx]\n",
        "\n",
        "    X_train = train2[~cv_idx]\n",
        "    y_train = y[~cv_idx]\n",
        "    \n",
        "    if sample_weight is not None:\n",
        "        y_train_weights = sample_weight[~cv_idx]\n",
        "        return X_train, y_train, X_test, y_test, y_train_weights\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a25PCDLHbcR"
      },
      "source": [
        "x = train.query('parentesco1==1')\n",
        "# X = train.copy()\n",
        "\n",
        "y = x['Target'] - 1\n",
        "x.drop('Target', axis=1, inplace=True)\n",
        "\n",
        "np.random.seed(seed=None)\n",
        "x_train, y_train, x_test, y_test = split_data(x, y, households=x['idhogar'].unique(), test_percentage=0.15)\n",
        "\n",
        "# 전체 데이터셋 학습\n",
        "x_train = x\n",
        "y_train = y\n",
        "\n",
        "train_households = x_train['idhogar']\n",
        "\n",
        "train_households = x_train.idhogar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZqp8LhFHfQG"
      },
      "source": [
        "# figure out the class weights for training with unbalanced classes\n",
        "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOUsbO0xHhJ_"
      },
      "source": [
        "# drop some features which aren't used by the LGBM or have very low importance\n",
        "extra_drop_features = [\n",
        " 'agg18_estadocivil1_MEAN',\n",
        " 'agg18_estadocivil6_COUNT',\n",
        " 'agg18_estadocivil7_COUNT',\n",
        " 'agg18_parentesco10_COUNT',\n",
        " 'agg18_parentesco11_COUNT',\n",
        " 'agg18_parentesco12_COUNT',\n",
        " 'agg18_parentesco1_COUNT',\n",
        " 'agg18_parentesco2_COUNT',\n",
        " 'agg18_parentesco3_COUNT',\n",
        " 'agg18_parentesco4_COUNT',\n",
        " 'agg18_parentesco5_COUNT',\n",
        " 'agg18_parentesco6_COUNT',\n",
        " 'agg18_parentesco7_COUNT',\n",
        " 'agg18_parentesco8_COUNT',\n",
        " 'agg18_parentesco9_COUNT',\n",
        " 'geo_elimbasu_LE_4',\n",
        " 'geo_energcocinar_LE_1',\n",
        " 'geo_energcocinar_LE_2',\n",
        " 'geo_epared_LE_0',\n",
        " 'geo_hogar_mayor',\n",
        " 'geo_manual_elec_LE_2',\n",
        " 'geo_pared_LE_3',\n",
        " 'geo_pared_LE_4',\n",
        " 'geo_pared_LE_5',\n",
        " 'geo_pared_LE_6',\n",
        " 'num_over_18',\n",
        " 'parentesco_LE',\n",
        " 'rez_esc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIv61OdRHi5W"
      },
      "source": [
        "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyc6vMECHken"
      },
      "source": [
        "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
        "\n",
        "# f1 score = recall, presision의 조화평균\n",
        "def evaluate_macroF1_lgb(predictions, truth):  \n",
        "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
        "    pred_labels = predictions.argmax(axis=1)\n",
        "    truth = truth.get_label()\n",
        "    f1 = f1_score(truth, pred_labels, average='macro')\n",
        "    return ('macroF1', 1-f1) \n",
        "\n",
        "fit_params={\"early_stopping_rounds\":500,\n",
        "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
        "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
        "            'verbose': False,\n",
        "           }\n",
        "\n",
        "def learning_rate_power_0997(current_iter):\n",
        "    base_learning_rate = 0.1\n",
        "    min_learning_rate = 0.02\n",
        "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
        "    return max(lr, min_learning_rate)\n",
        "\n",
        "fit_params['verbose'] = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxhOXI84NcH8"
      },
      "source": [
        "class VotingClassifierLGBM(VotingClassifier):\n",
        "    '''\n",
        "    This implements the fit method of the VotingClassifier propagating fit_params\n",
        "    '''\n",
        "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
        "        \n",
        "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
        "            raise NotImplementedError('Multilabel and multi-output'\n",
        "                                      ' classification is not supported.')\n",
        "\n",
        "        if self.voting not in ('soft', 'hard'):\n",
        "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
        "                             % self.voting)\n",
        "\n",
        "        if self.estimators is None or len(self.estimators) == 0:\n",
        "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
        "                                 ' should be a list of (string, estimator)'\n",
        "                                 ' tuples')\n",
        "\n",
        "        if (self.weights is not None and\n",
        "                len(self.weights) != len(self.estimators)):\n",
        "            raise ValueError('Number of classifiers and weights must be equal'\n",
        "                             '; got %d weights, %d estimators'\n",
        "                             % (len(self.weights), len(self.estimators)))\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            for name, step in self.estimators:\n",
        "                if not has_fit_parameter(step, 'sample_weight'):\n",
        "                    raise ValueError('Underlying estimator \\'%s\\' does not'\n",
        "                                     ' support sample weights.' % name)\n",
        "        names, clfs = zip(*self.estimators)\n",
        "        self._validate_names(names)\n",
        "\n",
        "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
        "        if n_isnone == len(self.estimators):\n",
        "            raise ValueError('All estimators are None. At least one is '\n",
        "                             'required to be a classifier!')\n",
        "\n",
        "        self.le_ = LabelEncoder().fit(y)\n",
        "        self.classes_ = self.le_.classes_\n",
        "        self.estimators_ = []\n",
        "\n",
        "        transformed_y = self.le_.transform(y)\n",
        "\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
        "                                                 sample_weight=sample_weight, **fit_params)\n",
        "                for clf in clfs if clf is not None)\n",
        "\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq_3qT2TO0In"
      },
      "source": [
        "np.random.seed(100)\n",
        "\n",
        "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
        "    estimator = clone(estimator1)\n",
        "    \n",
        "    # 데이터셋 무작위 분할\n",
        "    if sample_weight is not None:\n",
        "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
        "    else:\n",
        "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
        "        \n",
        "    # 분할된 데이터에 대한 새로운 fit_params\n",
        "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
        "    \n",
        "    # fit the estimator\n",
        "    if sample_weight is not None:\n",
        "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
        "            estimator.fit(X_train, y_train)\n",
        "        else:\n",
        "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
        "    else:\n",
        "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
        "            estimator.fit(X_train, y_train)\n",
        "        else:\n",
        "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
        "    \n",
        "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
        "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
        "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
        "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
        "    else:\n",
        "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
        "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
        "        print(\"Train F1:\", best_train)\n",
        "        print(\"Test F1:\", best_cv)\n",
        "        \n",
        "    # reject some estimators based on their performance on train and test sets\n",
        "    if threshold:\n",
        "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
        "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
        "            return estimator\n",
        "        # else recurse until we get a better one\n",
        "        else:\n",
        "            print(\"Unacceptable!!! Trying again...\")\n",
        "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
        "    \n",
        "    else:\n",
        "        return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJphp-2tHmYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb04cae-b222-4177-9343-f6d6a495df07"
      },
      "source": [
        "clfs = []\n",
        "for i in range(15):\n",
        "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
        "    clfs.append(('xgb{}'.format(i), clf))\n",
        "    \n",
        "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
        "del(clfs)\n",
        "\n",
        "# learning rate를 감소에 따라 최종 모델 학습\n",
        "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
        "\n",
        "clf_final = vc.estimators_[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.646101\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.578445\n",
            "[100]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.570407\n",
            "[150]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.583402\n",
            "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.583715\n",
            "[250]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583333\n",
            "[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583474\n",
            "Train F1: 0.9153725233385168\n",
            "Test F1: 0.4355385068644192\n",
            "[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.640044\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.583417\n",
            "[100]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.599753\n",
            "[150]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.593972\n",
            "[200]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.597668\n",
            "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.605199\n",
            "[299]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.594076\n",
            "Train F1: 0.8913538399552563\n",
            "Test F1: 0.42422233276003724\n",
            "[0]\tvalidation_0-merror:0.473064\tvalidation_0-macroF1:0.676192\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.593712\n",
            "[100]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.595794\n",
            "[150]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.589911\n",
            "[200]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.589821\n",
            "[250]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.597476\n",
            "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.596718\n",
            "Train F1: 0.8724441306732305\n",
            "Test F1: 0.42075548221923303\n",
            "[0]\tvalidation_0-merror:0.506734\tvalidation_0-macroF1:0.686421\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.531409\n",
            "[100]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.541944\n",
            "[150]\tvalidation_0-merror:0.361953\tvalidation_0-macroF1:0.543682\n",
            "[200]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.537668\n",
            "[250]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.530024\n",
            "[299]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.539738\n",
            "Train F1: 0.9054438067880621\n",
            "Test F1: 0.47823529311636404\n",
            "[0]\tvalidation_0-merror:0.488215\tvalidation_0-macroF1:0.690363\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.407407\tvalidation_0-macroF1:0.601575\n",
            "[100]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.591384\n",
            "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.593051\n",
            "[200]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.601453\n",
            "[250]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.604831\n",
            "[299]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.609499\n",
            "Train F1: 0.9108878975200805\n",
            "Test F1: 0.4173477361936218\n",
            "[0]\tvalidation_0-merror:0.459596\tvalidation_0-macroF1:0.633155\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.402357\tvalidation_0-macroF1:0.617957\n",
            "[100]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.614103\n",
            "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.604858\n",
            "[200]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.612738\n",
            "[250]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.609966\n",
            "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.610004\n",
            "Train F1: 0.8893336491231894\n",
            "Test F1: 0.4051414966703659\n",
            "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.667382\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.601972\n",
            "[100]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.605368\n",
            "[150]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.613276\n",
            "[200]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.620321\n",
            "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.61699\n",
            "[299]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.614077\n",
            "Train F1: 0.8972810131834246\n",
            "Test F1: 0.40066108970156944\n",
            "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.627544\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604169\n",
            "[100]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.601541\n",
            "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.604035\n",
            "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.606346\n",
            "[250]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.612084\n",
            "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.611192\n",
            "Train F1: 0.9110203375242469\n",
            "Test F1: 0.4069266211820104\n",
            "[0]\tvalidation_0-merror:0.405724\tvalidation_0-macroF1:0.569948\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.546907\n",
            "[100]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.55562\n",
            "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.55303\n",
            "[200]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.545353\n",
            "[250]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.546489\n",
            "[299]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.547837\n",
            "Train F1: 0.9023161714314298\n",
            "Test F1: 0.4669456786718976\n",
            "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.656075\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.60366\n",
            "[100]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.596851\n",
            "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.594956\n",
            "[200]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.59503\n",
            "[250]\tvalidation_0-merror:0.358586\tvalidation_0-macroF1:0.601222\n",
            "[299]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.60041\n",
            "Train F1: 0.891328566030215\n",
            "Test F1: 0.4133092608446235\n",
            "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.635594\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.598722\n",
            "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.586422\n",
            "[150]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572078\n",
            "[200]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.581793\n",
            "[250]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572344\n",
            "[299]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.580221\n",
            "Train F1: 0.9200869829778409\n",
            "Test F1: 0.42932198053495463\n",
            "[0]\tvalidation_0-merror:0.447811\tvalidation_0-macroF1:0.643927\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.591184\n",
            "[100]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604466\n",
            "[150]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.613042\n",
            "[200]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.611924\n",
            "[250]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.609384\n",
            "[299]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.618041\n",
            "Train F1: 0.9006803320768149\n",
            "Test F1: 0.4254007528480266\n",
            "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.615498\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.563908\n",
            "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.561588\n",
            "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.56399\n",
            "[200]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.564721\n",
            "[250]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.569799\n",
            "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.565604\n",
            "Train F1: 0.868783685054759\n",
            "Test F1: 0.45728526150302823\n",
            "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.599582\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.39899\tvalidation_0-macroF1:0.590061\n",
            "[100]\tvalidation_0-merror:0.40404\tvalidation_0-macroF1:0.60687\n",
            "[150]\tvalidation_0-merror:0.400673\tvalidation_0-macroF1:0.600754\n",
            "[200]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.597934\n",
            "[250]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.604446\n",
            "[299]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.601292\n",
            "Train F1: 0.8967214529450815\n",
            "Test F1: 0.4262237902576739\n",
            "[0]\tvalidation_0-merror:0.441077\tvalidation_0-macroF1:0.635276\n",
            "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
            "\n",
            "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
            "[50]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.564666\n",
            "[100]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.565395\n",
            "[150]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.569924\n",
            "[200]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566517\n",
            "[250]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.571177\n",
            "[299]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566762\n",
            "Train F1: 0.8935956630026125\n",
            "Test F1: 0.4472704708236759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFAeHObrNbAP",
        "outputId": "19bac42c-f98d-407f-ab82-54ac6c2a1926"
      },
      "source": [
        "# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\n",
        "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "vc.voting = 'soft'\n",
        "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "vc.voting = 'hard'\n",
        "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score of a single LGBM Classifier: 0.7870\n",
            "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8947\n",
            "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBtt22x2HpM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ab6090-7cbb-458d-e149-fa6bda151f86"
      },
      "source": [
        "# 어떤 모델에도 사용되지 않은 특성들\n",
        "useless_features = []\n",
        "drop_features = set()\n",
        "counter = 0\n",
        "for est in vc.estimators_:\n",
        "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
        "    useless_features.append(unused_features)\n",
        "    if counter == 0:\n",
        "        drop_features = set(unused_features)\n",
        "    else:\n",
        "        drop_features = drop_features.intersection(set(unused_features))\n",
        "    counter += 1\n",
        "    \n",
        "drop_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'agg18_estadocivil4_COUNT',\n",
              " 'agg18_estadocivil5_COUNT',\n",
              " 'geo_energcocinar_LE_0',\n",
              " 'geo_epared_LE_2',\n",
              " 'geo_manual_elec_LE_3'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxMA-U7lQdLU",
        "outputId": "d400026f-b15e-4fb7-ecbd-7ee28f0ee265"
      },
      "source": [
        "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature ranking:\n",
            "1. feature 59 (0.020874) - agg18_escolari_MAX\n",
            "2. feature 42 (0.019113) - fe_children_fraction\n",
            "3. feature 37 (0.017560) - SQBedjefe\n",
            "4. feature 126 (0.016810) - geo_sanitario_LE_3\n",
            "5. feature 74 (0.015333) - agg18_parentesco2_MEAN\n",
            "6. feature 46 (0.014168) - fe_human_bed_density\n",
            "7. feature 22 (0.013392) - dependency\n",
            "8. feature 60 (0.013138) - agg18_escolari_MEAN\n",
            "9. feature 40 (0.012836) - SQBdependency\n",
            "10. feature 135 (0.012545) - geo_pared_LE_7\n",
            "11. feature 112 (0.012330) - geo_etecho_LE_1\n",
            "12. feature 107 (0.011216) - geo_overcrowding\n",
            "13. feature 124 (0.010565) - geo_sanitario_LE_1\n",
            "14. feature 116 (0.010347) - geo_elimbasu_LE_0\n",
            "15. feature 19 (0.010287) - hogar_adul\n",
            "16. feature 34 (0.010142) - SQBescolari\n",
            "17. feature 105 (0.009962) - geo_hogar_total\n",
            "18. feature 23 (0.009942) - edjefe\n",
            "19. feature 94 (0.009886) - etecho_LE\n",
            "20. feature 17 (0.009594) - male\n",
            "21. feature 39 (0.009528) - SQBovercrowding\n",
            "22. feature 41 (0.009366) - SQBmeaned\n",
            "23. feature 44 (0.009305) - fe_all_man_fraction\n",
            "24. feature 96 (0.009216) - estadocivil_LE\n",
            "25. feature 6 (0.009131) - r4h1\n",
            "26. feature 97 (0.009013) - lugar_LE\n",
            "27. feature 65 (0.008972) - agg18_estadocivil3_MEAN\n",
            "28. feature 15 (0.008932) - cielorazo\n",
            "29. feature 16 (0.008922) - dis\n",
            "30. feature 120 (0.008912) - geo_elimbasu_LE_5\n",
            "31. feature 93 (0.008863) - epared_LE\n",
            "32. feature 106 (0.008822) - geo_bedrooms\n",
            "33. feature 104 (0.008821) - geo_hogar_adul\n",
            "34. feature 95 (0.008810) - eviv_LE\n",
            "35. feature 98 (0.008801) - tipovivi_LE\n",
            "36. feature 55 (0.008797) - agg18_age_MIN\n",
            "37. feature 12 (0.008775) - r4t1\n",
            "38. feature 92 (0.008711) - elimbasu_LE\n",
            "39. feature 131 (0.008691) - geo_manual_elec_LE_4\n",
            "40. feature 49 (0.008591) - fe_mobile_density\n",
            "41. feature 35 (0.008552) - SQBage\n",
            "42. feature 111 (0.008547) - geo_etecho_LE_0\n",
            "43. feature 27 (0.008483) - overcrowding\n",
            "44. feature 125 (0.008438) - geo_sanitario_LE_2\n",
            "45. feature 14 (0.008432) - escolari\n",
            "46. feature 5 (0.008364) - v18q1\n",
            "47. feature 33 (0.008358) - age\n",
            "48. feature 13 (0.008337) - r4t2\n",
            "49. feature 87 (0.008315) - piso_LE\n",
            "50. feature 86 (0.008305) - pared_LE\n",
            "51. feature 117 (0.008305) - geo_elimbasu_LE_1\n",
            "52. feature 25 (0.008275) - meaneduc\n",
            "53. feature 123 (0.008270) - geo_sanitario_LE_0\n",
            "54. feature 51 (0.008260) - fe_mobile_adult_density\n",
            "55. feature 69 (0.008140) - agg18_estadocivil5_MEAN\n",
            "56. feature 71 (0.008139) - agg18_estadocivil6_MEAN\n",
            "57. feature 72 (0.008112) - agg18_estadocivil7_MEAN\n",
            "58. feature 45 (0.008103) - fe_human_density\n",
            "59. feature 30 (0.008054) - qmobilephone\n",
            "60. feature 58 (0.008048) - agg18_escolari_MIN\n",
            "61. feature 43 (0.008048) - fe_working_man_fraction\n",
            "62. feature 102 (0.007966) - geo_dependency\n",
            "63. feature 0 (0.007906) - v2a1\n",
            "64. feature 128 (0.007870) - geo_manual_elec_LE_0\n",
            "65. feature 10 (0.007865) - r4m2\n",
            "66. feature 32 (0.007738) - area2\n",
            "67. feature 7 (0.007579) - r4h2\n",
            "68. feature 21 (0.007507) - hogar_total\n",
            "69. feature 109 (0.007466) - geo_eviv_LE_1\n",
            "70. feature 47 (0.007323) - fe_rent_per_person\n",
            "71. feature 142 (0.007268) - hhsize_to_rooms\n",
            "72. feature 63 (0.007225) - agg18_estadocivil2_MEAN\n",
            "73. feature 136 (0.007217) - bedrooms_to_rooms\n",
            "74. feature 1 (0.007192) - hacdor\n",
            "75. feature 138 (0.007158) - tamhog_to_rooms\n",
            "76. feature 50 (0.007148) - fe_tablet_density\n",
            "77. feature 2 (0.007079) - rooms\n",
            "78. feature 29 (0.007009) - television\n",
            "79. feature 91 (0.007006) - energcocinar_LE\n",
            "80. feature 99 (0.007003) - manual_elec_LE\n",
            "81. feature 57 (0.006932) - agg18_age_MEAN\n",
            "82. feature 62 (0.006899) - agg18_estadocivil1_COUNT\n",
            "83. feature 90 (0.006889) - sanitario_LE\n",
            "84. feature 100 (0.006777) - geo_age\n",
            "85. feature 31 (0.006771) - area1\n",
            "86. feature 61 (0.006766) - agg18_dis_MEAN\n",
            "87. feature 24 (0.006738) - edjefa\n",
            "88. feature 8 (0.006734) - r4h3\n",
            "89. feature 52 (0.006694) - fe_tablet_adult_density\n",
            "90. feature 143 (0.006689) - rent_to_hhsize\n",
            "91. feature 11 (0.006668) - r4m3\n",
            "92. feature 122 (0.006643) - geo_energcocinar_LE_3\n",
            "93. feature 48 (0.006560) - fe_rent_per_room\n",
            "94. feature 64 (0.006504) - agg18_estadocivil2_COUNT\n",
            "95. feature 56 (0.006441) - agg18_age_MAX\n",
            "96. feature 20 (0.006432) - hogar_mayor\n",
            "97. feature 26 (0.006391) - bedrooms\n",
            "98. feature 9 (0.006350) - r4m1\n",
            "99. feature 75 (0.006242) - agg18_parentesco3_MEAN\n",
            "100. feature 4 (0.006161) - refrig\n",
            "101. feature 18 (0.005876) - hogar_nin\n",
            "102. feature 101 (0.005755) - geo_meaneduc\n",
            "103. feature 38 (0.005725) - SQBhogar_nin\n",
            "104. feature 103 (0.005689) - geo_hogar_nin\n",
            "105. feature 28 (0.005391) - computer\n",
            "106. feature 140 (0.005380) - r4t3_to_rooms\n",
            "107. feature 88 (0.005258) - techo_LE\n",
            "108. feature 144 (0.005221) - rent_to_over_18\n",
            "109. feature 67 (0.005210) - agg18_estadocivil4_MEAN\n",
            "110. feature 110 (0.005132) - geo_eviv_LE_2\n",
            "111. feature 129 (0.005069) - geo_manual_elec_LE_1\n",
            "112. feature 53 (0.005035) - fe_people_not_living\n",
            "113. feature 79 (0.004939) - agg18_parentesco7_MEAN\n",
            "114. feature 77 (0.004880) - agg18_parentesco5_MEAN\n",
            "115. feature 137 (0.004457) - rent_to_rooms\n",
            "116. feature 114 (0.004299) - geo_epared_LE_1\n",
            "117. feature 81 (0.004282) - agg18_parentesco9_MEAN\n",
            "118. feature 141 (0.004248) - v2a1_to_r4t3\n",
            "119. feature 83 (0.004106) - agg18_parentesco11_MEAN\n",
            "120. feature 133 (0.003824) - geo_pared_LE_1\n",
            "121. feature 3 (0.003191) - hacapo\n",
            "122. feature 78 (0.002986) - agg18_parentesco6_MEAN\n",
            "123. feature 84 (0.002735) - agg18_parentesco12_MEAN\n",
            "124. feature 89 (0.002506) - abastagua_LE\n",
            "125. feature 139 (0.001633) - r4t3_to_tamhog\n",
            "126. feature 76 (0.001460) - agg18_parentesco4_MEAN\n",
            "127. feature 80 (0.000000) - agg18_parentesco8_MEAN\n",
            "128. feature 132 (0.000000) - geo_pared_LE_0\n",
            "129. feature 119 (0.000000) - geo_elimbasu_LE_3\n",
            "130. feature 36 (0.000000) - SQBhogar_total\n",
            "131. feature 82 (0.000000) - agg18_parentesco10_MEAN\n",
            "132. feature 73 (0.000000) - agg18_parentesco1_MEAN\n",
            "133. feature 118 (0.000000) - geo_elimbasu_LE_2\n",
            "134. feature 130 (0.000000) - geo_manual_elec_LE_3\n",
            "135. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
            "136. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
            "137. feature 115 (0.000000) - geo_epared_LE_2\n",
            "138. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
            "139. feature 85 (0.000000) - edjef\n",
            "140. feature 121 (0.000000) - geo_energcocinar_LE_0\n",
            "141. feature 54 (0.000000) - fe_people_weird_stat\n",
            "142. feature 108 (0.000000) - geo_eviv_LE_0\n",
            "143. feature 134 (0.000000) - geo_pared_LE_2\n",
            "144. feature 113 (0.000000) - geo_etecho_LE_2\n",
            "145. feature 127 (0.000000) - geo_sanitario_LE_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmzD74V3QpH8"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny7TBxncQeta"
      },
      "source": [
        "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
        "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
        "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
        "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
        "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
        "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
        "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
        "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
        "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
        "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
        "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
        "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
        "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
        "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
        "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
        "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
        "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
        "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
        "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
        "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
        "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
        "\n",
        "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
        "       'fe_tablet_adult_density', 'fe_tablet_density'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxVxATX2QwM_",
        "outputId": "7c4a4228-4d7f-45a7-a0c0-8a2739fd4d30"
      },
      "source": [
        "# do the same thing for some extra trees classifiers\n",
        "ets = []    \n",
        "for i in range(10):\n",
        "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
        "    ets.append(('rf{}'.format(i), rf))   \n",
        "\n",
        "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
        "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train F1: 0.8990704054631374\n",
            "Test F1: 0.42315372204859547\n",
            "Train F1: 0.897995418026043\n",
            "Test F1: 0.40719052936014105\n",
            "Train F1: 0.8963498800724583\n",
            "Test F1: 0.3974529129044045\n",
            "Train F1: 0.8928010305266193\n",
            "Test F1: 0.40588786789538295\n",
            "Train F1: 0.8909343687879617\n",
            "Test F1: 0.46641624834654005\n",
            "Train F1: 0.8938464106016324\n",
            "Test F1: 0.4311750086470195\n",
            "Train F1: 0.8946332696838437\n",
            "Test F1: 0.41657466647986424\n",
            "Train F1: 0.898467062343887\n",
            "Test F1: 0.4595226009798874\n",
            "Train F1: 0.9018704101003249\n",
            "Test F1: 0.42369826721485787\n",
            "Train F1: 0.8929741191384623\n",
            "Test F1: 0.43477646256408004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGE0dPwTQ1Bf",
        "outputId": "e83b4565-535b-4e3e-adb3-aa367c9b1329"
      },
      "source": [
        "# w/ threshold, extra drop cols\n",
        "vc2.voting = 'soft'\n",
        "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "vc2.voting = 'hard'\n",
        "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8274\n",
            "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Q94ikEQ1tR",
        "outputId": "8c9e0e67-4dd8-420d-ae0b-842d174025b4"
      },
      "source": [
        "# w/o threshold, extra drop cols\n",
        "vc2.voting = 'soft'\n",
        "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "vc2.voting = 'hard'\n",
        "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8274\n",
            "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN3jviM1Q3Ih",
        "outputId": "63800b1d-4e6e-49a5-ede6-690ee824848e"
      },
      "source": [
        "# see which features are not used by ANY models\n",
        "useless_features = []\n",
        "drop_features = set()\n",
        "counter = 0\n",
        "for est in vc2.estimators_:\n",
        "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
        "    useless_features.append(unused_features)\n",
        "    if counter == 0:\n",
        "        drop_features = set(unused_features)\n",
        "    else:\n",
        "        drop_features = drop_features.intersection(set(unused_features))\n",
        "    counter += 1\n",
        "    \n",
        "drop_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'parentesco_LE', 'rez_esc'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbpYf9BTQ6-f"
      },
      "source": [
        "def combine_voters(data, weights=[0.5, 0.5]):\n",
        "    # do soft voting with both classifiers\n",
        "    vc.voting=\"soft\"\n",
        "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
        "    vc2.voting=\"soft\"\n",
        "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
        "    \n",
        "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
        "    predictions = np.argmax(final_vote, axis=1)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b64ZY-veQ89N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087db86a-341f-473d-b829-3e4e3e77693a"
      },
      "source": [
        "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
        "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8757819242291919"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8e8swNcROLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c53ebd-30b7-4381-8a11-a01c14941eee"
      },
      "source": [
        "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
        "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8686167738435305"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN5GZfq1RQnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b3345f-958d-484c-d3c8-a764d8982df8"
      },
      "source": [
        "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
        "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.884723015840821"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKUEH1zmRW9Q"
      },
      "source": [
        "y_subm = pd.DataFrame()\n",
        "y_subm['Id'] = test_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAjkw_SCRZ38"
      },
      "source": [
        "vc.voting = 'soft'\n",
        "y_subm_lgb = y_subm.copy(deep=True)\n",
        "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
        "\n",
        "vc2.voting = 'soft'\n",
        "y_subm_rf = y_subm.copy(deep=True)\n",
        "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
        "\n",
        "y_subm_ens = y_subm.copy(deep=True)\n",
        "y_subm_ens['Target'] = combine_voters(test) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwS0C5HJRbQs"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "\n",
        "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "\n",
        "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
        "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
        "y_subm_ens.to_csv(sub_file_ens, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeCRyViNRnVD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}